{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "819e7379"
   },
   "source": [
    "# Transformer\n",
    "在这个notebook中，我们将逐步实现 Transformer 模型，参考原始论文[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)。我们还将使用一个AddSub数据集来解决一个向量到向量的问题。\n",
    "\n",
    "在序列到序列的任务中，通常我们会同时使用编码器和解码器。在这次作业中，你将学习如何逐步实现基于编码器-解码器的 Transformer。我们将在这里实现一个更简单的版本，完成一个向量到向量的任务。这基本上意味着输入和输出序列的长度是**固定的**，我们不必担心输入序列的可变长度。\n",
    "\n",
    "1. [**第一部分（准备工作）**](#第一部分：准备工作)：导入需要的包，运行项目中所需要的function\n",
    "1. [**第二部分（实现 Transformer 模块）**](#第二部分：实现Transformer构建模块)：我们将学习如何实现 Transformer 的构建模块。它将包括以下模块：\n",
    "   1. 多头注意力（MultiHeadAttention）\n",
    "   2. 前馈网络（FeedForward）\n",
    "   3. 层归一化（LayerNorm）\n",
    "   4. 编码器块（Encoder Block）\n",
    "   5. 解码器块（Decoder Block）\n",
    "1. [**第三部分（数据加载）**](#第三部分：数据加载器)：我们将预处理一个AddSub数据集，其中包含输入算术表达式和表达式的输出结果。我们将使用位置编码（Positional Embedding）模块，同时我们将会构建数据加载器（DataLoader）。\n",
    "1. [**第四部分（训练模型）**](#第四部分:在AddSub数据集上训练Transformer)：在最后一部分，我们将看看如何将实现的 Transformer 模型拟合到AddSub数据集上。\n",
    "\n",
    "你可以在 CPU 上运行所有事情直到第3部分。第4部分需要 GPU，而在切换到此部分的运行时时，你还需要运行所有之前的部分，因为第4部分依赖于之前的部分。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3e1c7486"
   },
   "source": [
    "# 第一部分：准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请同学们运行一下block，导入所需要的包以及功能函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:20:21.413831Z",
     "iopub.status.busy": "2024-04-24T07:20:21.413210Z",
     "iopub.status.idle": "2024-04-24T07:20:21.423155Z",
     "shell.execute_reply": "2024-04-24T07:20:21.422350Z",
     "shell.execute_reply.started": "2024-04-24T07:20:21.413798Z"
    },
    "id": "2cbf5f86",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 导入所需的包\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import Image\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import seaborn\n",
    "import os\n",
    "\n",
    "\n",
    "# for plotting\n",
    "% matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0)  # set default size of plots\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b807888a"
   },
   "source": [
    "我们将在第四部分使用GPU，前三个部分可以不开启GPU加速。\n",
    "在第四部分，我们将使用 GPU 来加速我们的计算，运行此单元格以确保你正在使用 GPU。\n",
    "\n",
    "我们将使用 `torch.float = torch.float32` 来表示数据，以及 `torch.long = torch.int64` 来表示标签。\n",
    "\n",
    "请参阅 [torch.dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch-dtype) 了解有关数据类型的更多细节。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:20:28.176998Z",
     "iopub.status.busy": "2024-04-24T07:20:28.176601Z",
     "iopub.status.idle": "2024-04-24T07:20:28.238724Z",
     "shell.execute_reply": "2024-04-24T07:20:28.237692Z",
     "shell.execute_reply.started": "2024-04-24T07:20:28.176966Z"
    },
    "id": "0150e9a5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "to_float = torch.float\n",
    "to_long = torch.long\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Good to go!\")\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Please set GPU via Edit -> Notebook Settings.\")\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "项目中需要用到的一些function，请运行一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:20:31.236612Z",
     "iopub.status.busy": "2024-04-24T07:20:31.235699Z",
     "iopub.status.idle": "2024-04-24T07:20:31.243349Z",
     "shell.execute_reply": "2024-04-24T07:20:31.242328Z",
     "shell.execute_reply.started": "2024-04-24T07:20:31.236579Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def reset_seed(number):\n",
    "    \"\"\"\n",
    "    重置随机种子为指定的数字\n",
    "\n",
    "    输入：\n",
    "    - number：要使用的种子数\n",
    "    \"\"\"\n",
    "    random.seed(number)\n",
    "    torch.manual_seed(number)\n",
    "    return\n",
    "\n",
    "def rel_error(x, y, eps=1e-10):\n",
    "    \"\"\"\n",
    "    这个函数用于计算两个张量 x 和 y 之间的相对误差。相对误差被定义为：\n",
    "    \n",
    "                            max_i |x_i - y_i]|\n",
    "    rel_error(x, y) = -------------------------------\n",
    "                      max_i |x_i| + max_i |y_i| + eps\n",
    "\n",
    "    输入：\n",
    "    - x，y：具有相同形状的张量\n",
    "    - eps：用于数值稳定性的小正常数\n",
    "\n",
    "    返回：\n",
    "    - rel_error：给出 x 和 y 之间相对误差的标量\n",
    "    \"\"\"\n",
    "    top = (x - y).abs().max().item()\n",
    "    bot = (x.abs() + y.abs()).clamp(min=eps).max().item()\n",
    "    return top / bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58aed0ae"
   },
   "source": [
    "# 第二部分：实现Transformer构建模块\n",
    "\n",
    "在这一部分，我们将看到如何实现用于构建 Transformer 模型的各种模块。然后我们将使用这些模块来构建 Transformer 编码器和解码器，最终实现完整的 Transformer 模型。\n",
    "每个模块都将被实现为 `nn.Module` 的子类；我们将使用 PyTorch 的自动微分来计算梯度，因此不需要手动实现反向传播。\n",
    "\n",
    "我们将按照原始论文的参考来实现以下模块：\n",
    "\n",
    "1. 多头注意力（MultiHeadAttention）模块\n",
    "2. 前馈网络（FeedForward）模块\n",
    "3. 层归一化（Layer Normalization）\n",
    "4. 位置编码（Positional Encoding）模块\n",
    "\n",
    "然后，我们将使用这些构建模块，结合输入嵌入层，来构建 Transformer 编码器和解码器。我们将从实现多头注意力（MultiHeadAttention）、前馈网络（FeedForward）和层归一化（Layer Normalization）开始，稍后再看位置编码和输入嵌入。 \n",
    "\n",
    "**注意：** 在实现这些模块时，所有这些模块的输入和输出张量的形状应该相同,可以通过检查检查输入和输出张量的形状判断代码补全是否正确。"
   ]
  },
  {
   "attachments": {
    "1363dc0d-1aa1-481a-af9e-2ceaf1041249.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多头注意力模块\n",
    "\n",
    "Transformers是一种序列到序列的网络，它接收一个序列（例如英文句子）作为输入，输出一个序列（例如西班牙文句子）。输入序列首先被转换为嵌入向量，类似于我们在RNN部分讨论的内容，然后这些嵌入向量被传递到一个位置编码模块。接着，这些嵌入向量被转换为三个向量：查询（query）、键（key）和值（value），这个转换过程使用的是可学习的权重。然后我们使用一个Transformer编码器和解码器来得到最终的输出序列。\n",
    "\n",
    "![image.png](attachment:1363dc0d-1aa1-481a-af9e-2ceaf1041249.png)\n",
    "\n",
    "在这一部分，我们假设我们已经得到了查询、键和值向量(即我们需要处理的输入是如上图所示的$Q，K，V$)，并且我们将对它们进行处理。为了实现基本的多头注意力模块，我们首先将实现自注意力模块，然后从自注意力模块扩展到多头注意力模块。\n",
    "\n",
    "### 自注意力模块\n",
    "\n",
    "受到信息检索范式的启发，Transformers引入了查询、键和值(*query*、*key* 和*value*)的概念。在给定一个 *query* 的情况下，我们尝试从 *key*-*value* 对中提取信息。按照这些思路，我们通过对每个 *query* 计算 *key* 的加权和来数学上实现这一过程，其中权重由 *query* 和 *key* 的点积计算得到。更确切地说，对于每个 *query* ，我们计算其与所有 *key* 的点积，然后使用这些点积的标量输出作为权重，以找到 *values* 的加权和。请注意，在找到加权和之前，我们还会对权重向量应用 softmax 函数。我们将实现一个注意力模块，它接收查询、键和值向量作为输入，并返回一个张量，即值的加权和。\n",
    "\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "其中，$Q$、$K$ 和 $V$ 分别代表查询（Query）、键（Key）和值（Value）。这些都是通过对输入数据进行线性变换得到的。$d_k$ 是键向量的维度。\n",
    "\n",
    "这个公式的含义是：\n",
    "\n",
    "1. 计算查询和所有键的点积，得到一个得分。这个得分表示查询和每个键的匹配程度。\n",
    "\n",
    "2. 除以 $\\sqrt{d_k}$ 是进行scale操作。\n",
    "\n",
    "3. 通过 softmax 函数，将得分转换为概率分布。这个概率分布表示模型在每个位置的“注意力”。\n",
    "\n",
    "4. 最后，这个注意力分布和值向量相乘，得到最终的输出。这个输出是一个加权和，其中的权重就是注意力分布。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "在这一部分中，你需要实现`scaled_dot_product_no_loop_batch`函数，请**不要用任何loop语句**。\n",
    "\n",
    "可以参考使用:\n",
    "1. [torch.bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
    "2. [torch.softmax](https://pytorch.org/docs/stable/generated/torch.softmax.html)\n",
    "3. [转置](https://pytorch.org/docs/stable/generated/torch.transpose.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码填空1: 实现注意力计算（12分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_no_loop_batch(\n",
    "    query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None\n",
    ") -> Tensor:\n",
    "    \"\"\"\n",
    "    该函数执行注意力机制的基本块，即缩放点积。我们将输入的 query、key 和 value 映射到输出。\n",
    "    它使用矩阵-矩阵乘法来找到缩放权重，然后使用矩阵-矩阵乘法来找到最终输出。\n",
    "\n",
    "    参数：\n",
    "        query：形状为 (N,K, M) 的张量，其中 N 是批次大小，K 是序列长度，M 是序列嵌入维度\n",
    "        key：形状为 (N, K, M) 的张量，其中 N 是批次大小，K 是序列长度，M 是序列嵌入维度\n",
    "        value：形状为 (N, K, M) 的张量，其中 N 是批次大小，K 是序列长度，M 是序列嵌入维度\n",
    "        mask：形状为 (N, K, K) 的布尔张量，用于掩盖计算加权和的权重\n",
    "\n",
    "    返回：\n",
    "        y：形状为 (N, K, M) 的张量，包含值的加权和\n",
    "        weights_softmax：形状为 (N, K, K) 的张量，包含 softmax 后的权重矩阵。\n",
    "\n",
    "    \"\"\"\n",
    "    _, _, M = query.shape\n",
    "    ###############################################################################\n",
    "    # TODO: 该函数执行自注意力操作， 请不要用任何loop实现。 \n",
    "    ###############################################################################\n",
    "    scores = # TODO: 计算query与key的点积，张量之间的乘法请参考(1)torch.bmm (2)张量的转置操作\n",
    "    if mask is not None: # 将掩码应用于权重矩阵，将 -1e9 赋值给掩码值为 True 的位置，否则保持不变\n",
    "        scores = scores.masked_fill(mask, -1e9)\n",
    "    weights = # TODO： 对权重应用softmax函数，请参考torch.softmax\n",
    "    y = # TODO: 作为权重的标量和value进行加权\n",
    "    return y, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试代码1: 测试注意力代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b13b2d96",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "N = 2  # 句子数量\n",
    "K = 5  # 每个句子里的单词数\n",
    "M = 4  # 每个单词embedding的特征维度\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
    "\n",
    "\n",
    "y, _ = scaled_dot_product_no_loop_batch(query, key, value)\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.09603, -0.06782, -0.03962, -0.01141],\n",
    "            [-0.08991, -0.06170, -0.03350, -0.00529],\n",
    "            [-0.08376, -0.05556, -0.02735, 0.00085],\n",
    "            [-0.07760, -0.04939, -0.02119, 0.00702],\n",
    "            [-0.07143, -0.04322, -0.01502, 0.01319],\n",
    "        ],\n",
    "        [\n",
    "            [0.49884, 0.52705, 0.55525, 0.58346],\n",
    "            [0.50499, 0.53319, 0.56140, 0.58960],\n",
    "            [0.51111, 0.53931, 0.56752, 0.59572],\n",
    "            [0.51718, 0.54539, 0.57359, 0.60180],\n",
    "            [0.52321, 0.55141, 0.57962, 0.60782],\n",
    "        ],\n",
    "    ]\n",
    ").to(torch.float32)\n",
    "\n",
    "if y.shape == y_expected.shape:\n",
    "    print(f'Shapes of y and y_expected are the same')\n",
    "else:\n",
    "    print('Check the output shape of your funtion')\n",
    "print(\"scaled_dot_product_no_loop_batch error: \", rel_error(y_expected, y))\n",
    "if rel_error(y_expected, y) > 1e-5:\n",
    "    print('There are some problems with your code, please check!')\n",
    "else:\n",
    "    print('The error is less than 1e-5. Done!')\n",
    "\n",
    "# Error应该小于1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56e21e2d"
   },
   "source": [
    "### 观察时间复杂度：\n",
    "\n",
    "Transformers 时间复杂度取决于输入序列的大小。\n",
    "现在我们已经实现了 `self_attention_no_loop`，我们可以验证这一点。\n",
    "运行下面的单元格：第一个具有长度为 256 的序列，第二个具有长度为 512 的序列。您应该会发现长度为 512 的情况大约慢 4 倍，从而显示出 Transformers 的复杂度随着序列长度的增加呈二次增长。\n",
    "`%timeit` 行可能需要几秒钟才能运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7d58596",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "N = 64\n",
    "K = 256  \n",
    "M = emb_size = 2048\n",
    "dim_q = dim_k = 2048\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
    "\n",
    "%timeit -n 5 -r 2  y = scaled_dot_product_no_loop_batch(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a85adf69",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "N = 64\n",
    "K = 512  \n",
    "M = emb_size = 2048\n",
    "dim_q = dim_k = 2048\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M).reshape(N, K, M)  # **to_double_cuda\n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M).reshape(N, K, M)  # *to_double_cuda\n",
    "\n",
    "%timeit -n 5 -r 2  y = scaled_dot_product_no_loop_batch(query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d6e92b0"
   },
   "source": [
    "#### 代码填空2: 自注意力层的实现（8分）\n",
    "\n",
    "现在我们已经实现了 `scaled_dot_product_no_loop_batch`，让我们实现 `SingleHeadAttention`，它将作为 `MultiHeadAttention` 块的构建模块。对于这个练习，我们创建了一个 `SingleHeadAttention` 类，它继承自 PyTorch 的 `nn.module` 类。你需要 `forward` 函数。\n",
    "\n",
    "自注意力层中，我们初始化的三个linear层，即为$W^q, W^k, W^v$权重。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_v: int):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        该类封装了自注意力层的实现。我们使用 MLP 层映射输入的查询、键和值，\n",
    "        然后使用 scaled_dot_product_no_loop_batch 得到最终输出。\n",
    "\n",
    "        参数：\n",
    "            dim_in：输入序列嵌入维度的整数值\n",
    "            dim_q：查询和键向量输出维度的整数值\n",
    "            dim_v：值向量的输出维度的整数值\n",
    "\n",
    "        \"\"\"\n",
    "        ##########################################################################\n",
    "        # 初始化三个 nn.Linear 层，可以将输入维度为 dim_in 的输入转换为维度为 dim_q 的查询， #\n",
    "        # 维度为 dim_q 的键，和维度为 dim_v 的值。                #\n",
    "        ##########################################################################\n",
    "        # 初始化q, k, v， 由于qk要进行点积计算，其维度需要保持一致\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_q)\n",
    "        self.v = nn.Linear(dim_in, dim_v)\n",
    "        self.weights_softmax = None\n",
    "        # 对于每个 Linear 层，使用以下策略初始化权重： #\n",
    "        # 如果一个 Linear 层具有输入维度 D_in 和输出维度 D_out，则将权重初始化为从 [-c, c]  #\n",
    "        # 范围内的均匀分布中抽取的值，其中 c = sqrt(6/(D_in + D_out))。   \n",
    "        self.q.weight.data.uniform_(-math.sqrt(6/(dim_in+dim_q)), math.sqrt(6/(dim_in+dim_q)))\n",
    "        self.k.weight.data.uniform_(-math.sqrt(6/(dim_in+dim_q)), math.sqrt(6/(dim_in+dim_q)))\n",
    "        self.v.weight.data.uniform_(-math.sqrt(6/(dim_in+dim_v)), math.sqrt(6/(dim_in+dim_v)))\n",
    "\n",
    "    def forward(\n",
    "        self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None\n",
    "    ) -> Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        该函数执行自注意力层的前向传播。\n",
    "\n",
    "        参数：\n",
    "            query：形状为 (N, K, M) 的张量\n",
    "            key：形状为 (N, K, M) 的张量\n",
    "            value：形状为 (N, K, M) 的张量\n",
    "            mask：形状为 (N, K, K) 的张量\n",
    "        返回：\n",
    "            y：形状为 (N, K, dim_v) 的张量\n",
    "        \"\"\"\n",
    "        ##########################################################################\n",
    "        # TODO: 使用 init 函数中初始化的函数找到输出张量。确切地说，将输入 query、key 和 value #\n",
    "        # 传递给上面初始化的三个函数。然后，将这三个转换后的 query、key 和 value 张量传递给 #\n",
    "        # self_attention_no_loop_batch，以获得最终输出。只需将其作为 self_attention_no_loop_batch #\n",
    "        # 中的一个变量传递。将 self_attention_no_loop_batch 的输出权重矩阵值赋给变量 self.weights_softmax。#\n",
    "        ##########################################################################\n",
    "        # 应用self_attention_no_loop_batch\n",
    "        # y和self.weights_softmax为self_attention_no_loop_batch 应用后的输出\n",
    "        # TODO: 添加你的代码\n",
    "        Q = # TODO\n",
    "        K = # TODO\n",
    "        V = # TODO\n",
    "        y, self.weights_softmax = # TODO\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试代码2: 自注意力层的实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "K = 4\n",
    "M = emb_size = 4\n",
    "dim_q = dim_k = 4\n",
    "atten_single = SelfAttention(emb_size, dim_q, dim_k)\n",
    "\n",
    "for k, v in atten_single.named_parameters():\n",
    "    # print(k, v.shape) \n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  \n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  \n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  \n",
    "\n",
    "query.retain_grad()\n",
    "key.retain_grad()\n",
    "value.retain_grad()\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-1.10382, -0.37219, 0.35944, 1.09108],\n",
    "            [-1.45792, -0.50067, 0.45658, 1.41384],\n",
    "            [-1.74349, -0.60428, 0.53493, 1.67414],\n",
    "            [-1.92584, -0.67044, 0.58495, 1.84035],\n",
    "        ],\n",
    "        [\n",
    "            [-4.59671, -1.63952, 1.31767, 4.27486],\n",
    "            [-4.65586, -1.66098, 1.33390, 4.32877],\n",
    "            [-4.69005, -1.67339, 1.34328, 4.35994],\n",
    "            [-4.71039, -1.68077, 1.34886, 4.37848],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "dy_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.09084, -0.08961, -0.08838, -0.08715],\n",
    "            [0.69305, 0.68366, 0.67426, 0.66487],\n",
    "            [-0.88989, -0.87783, -0.86576, -0.85370],\n",
    "            [0.25859, 0.25509, 0.25158, 0.24808],\n",
    "        ],\n",
    "        [\n",
    "            [-0.05360, -0.05287, -0.05214, -0.05142],\n",
    "            [0.11627, 0.11470, 0.11312, 0.11154],\n",
    "            [-0.01048, -0.01034, -0.01019, -0.01005],\n",
    "            [-0.03908, -0.03855, -0.03802, -0.03749],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "\n",
    "y = atten_single(query, key, value)\n",
    "dy = torch.randn(*y.shape)  \n",
    "\n",
    "y.backward(dy)\n",
    "query_grad = query.grad\n",
    "\n",
    "print(\"SelfAttention error: \", rel_error(y_expected, y))\n",
    "print(\"SelfAttention error: \", rel_error(dy_expected, query_grad))\n",
    "if rel_error(y_expected, y) > 1e-5 and rel_error(dy_expected, query_grad) > 1e-5:\n",
    "    print('There are some problems with your code, please check!')\n",
    "else:\n",
    "    print('The error is less than 1e-5. Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4b80b79"
   },
   "source": [
    "#### 代码填空3: 多头注意力（15分）\n",
    "\n",
    "我们已经实现了 `SingleHeadAttention` 块，这让我们非常接近实现 `MultiHeadAttention`。现在我们将看到，通过根据 Multi-Attention 块中的头数来操作输入张量的形状，可以实现这一点。我们设计了一个网络，该网络在相同的输入上使用多个 SingleHeadAttention 块来计算输出张量，并最终将它们连接起来生成单个输出。这不是实际使用的实现方式，因为它强制你初始化多个层，但我们在这里出于简单起见，使用 `SingleHeadAttention` 块实现 `MultiHeadAttention` 块。\n",
    "\n",
    "可以参考：\n",
    "1. [nn.ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html)\n",
    "2. [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "3. [torch.cat](https://pytorch.org/docs/stable/generated/torch.cat.html#torch-cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6da2558e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_out: int):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        一个简单的 Transformer 模型中 MultiheadAttention 层的实现。\n",
    "        我们在相同的输入上并行使用多个 SelfAttention 层，然后将它们连接成单个张量。\n",
    "        然后通过 MLP 将该张量传递以生成最终输出。\n",
    "        输入形状将为 (N, K, M)，其中 N 是批量大小，K 是序列长度，M 是序列嵌入维度。\n",
    "        参数：\n",
    "            num_heads：指定头数的整数值\n",
    "            dim_in：指定查询、键和值的输入维度的整数值。这将是每个 SingleHeadAttention 块的输入维度\n",
    "            dim_out：指定完整 MultiHeadAttention 块的输出维度的整数值\n",
    "\n",
    "\n",
    "\n",
    "        注意：在这里，当我们说维度时，我们指的是嵌入的维度。\n",
    "              在 Transformers 中，输入是形状为 (N, K, M) 的张量，这里 N 是批量大小，K 是序列长度，\n",
    "              而 M 是输入嵌入的大小。由于序列长度(K)和批量数(N)通常不会改变，我们通常只转换\n",
    "              维度(M) 维度。\n",
    "        \"\"\"\n",
    "        ##########################################################################\n",
    "        # TODO: 在这里初始化两个东西：                                              #\n",
    "        # 1.) 使用 nn.ModuleList 初始化一系列 SingleHeadAttention 层模块。          #\n",
    "        #    这个列表的长度应该等于 num_heads，每个 SingleHeadAttention 层的输入维度为 dim_in， #\n",
    "        #    查询、键和值的维度均为 dim_out。                                       #\n",
    "        # 2.) 使用 nn.Linear 将 nn.Modulelist 块的输出映射回 dim_in。               #\n",
    "        ##########################################################################\n",
    "        self.heads = # TODO: self.heads = nn.ModuleList('add your code here')\n",
    "        self.linear = # TODO: nn.Linear\n",
    "        nn.init.uniform_(self.linear.weight, -torch.sqrt(torch.tensor(6. / (num_heads * dim_out + dim_in))), torch.sqrt(torch.tensor(6. / (num_heads * dim_out + dim_in))))# 权重初始化\n",
    "\n",
    "    def forward(\n",
    "        self, query: Tensor, key: Tensor, value: Tensor, mask: Tensor = None\n",
    "    ) -> Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        MultiHeadAttention 层的前向传播的实现。\n",
    "\n",
    "        参数：\n",
    "            query：形状为 (N, K, M) 的张量，其中 N 是批量中的序列数，\n",
    "                K 是序列长度，M 是输入嵌入维度。M 应与初始化函数中的 dim_in 相等\n",
    "\n",
    "            key：形状为 (N, K, M) 的张量，其中 N 是批量中的序列数，\n",
    "                K 是序列长度，M 是输入嵌入维度。M 应与初始化函数中的 dim_in 相等\n",
    "\n",
    "            value：形状为 (N, K, M) 的张量，其中 N 是批量中的序列数，\n",
    "                K 是序列长度，M 是输入嵌入维度。M 应与初始化函数中的 dim_in 相等\n",
    "\n",
    "            mask：形状为 (N, K, K) 的张量，其中 N 是批量中的序列数，\n",
    "                K 是序列长度，M 是输入嵌入维度。M 应与初始化函数中的 dim_in 相等\n",
    "\n",
    "        返回：\n",
    "            y：形状为 (N, K, M) 的张量\n",
    "        \"\"\"\n",
    "        y = None\n",
    "        ##########################################################################\n",
    "        # TODO: 使用初始化函数中定义的变量，通过 MultiHeadAttention 块进行前向传播。\n",
    "        # nn.ModuleList 的行为类似一个列表，你可以使用 for 循环或列表解析来提取其中的不同元素。\n",
    "        # nn.ModuleList 中的每个元素都是一个 SingleHeadAttention，它将使用相同的查询、键、值和掩码张量，\n",
    "        # 并且你将得到一个张量列表作为输出。将这个张量列表连接起来，然后通过\n",
    "        # nn.Linear 映射函数进行处理。                                        #\n",
    "        ##########################################################################\n",
    "        outputs = []\n",
    "        for head in self.heads:\n",
    "            output_head = # TODO\n",
    "            outputs.append(output_head)\n",
    "        concat = torch.cat(outputs, dim=2) # torch.cat\n",
    "        y = # TODO\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试代码3: 多头注意力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87bf04ff",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "num_heads = 2\n",
    "K = 4\n",
    "M = inp_emb_size = 4\n",
    "out_emb_size = 8\n",
    "atten_multihead = MultiHeadAttention(num_heads, inp_emb_size, out_emb_size)\n",
    "\n",
    "for k, v in atten_multihead.named_parameters():\n",
    "#     print(f'key of parameter:{k}, shape of its value:{v.shape}') \n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "query = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  \n",
    "key = torch.linspace(-0.8, 0.5, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  \n",
    "value = torch.linspace(-0.3, 0.8, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  \n",
    "\n",
    "query.retain_grad()\n",
    "key.retain_grad()\n",
    "value.retain_grad()\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [-0.23104, 0.50132, 1.23367, 1.96603],\n",
    "            [0.68324, 1.17869, 1.67413, 2.16958],\n",
    "            [1.40236, 1.71147, 2.02058, 2.32969],\n",
    "            [1.77330, 1.98629, 2.19928, 2.41227],\n",
    "        ],\n",
    "        [\n",
    "            [6.74946, 5.67302, 4.59659, 3.52015],\n",
    "            [6.82813, 5.73131, 4.63449, 3.53767],\n",
    "            [6.86686, 5.76001, 4.65315, 3.54630],\n",
    "            [6.88665, 5.77466, 4.66268, 3.55070],\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "dy_expected = torch.tensor(\n",
    "    [[[ 0.56268,  0.55889,  0.55510,  0.55131],\n",
    "         [ 0.43286,  0.42994,  0.42702,  0.42411],\n",
    "         [ 2.29865,  2.28316,  2.26767,  2.25218],\n",
    "         [ 0.49172,  0.48841,  0.48509,  0.48178]],\n",
    "\n",
    "        [[ 0.25083,  0.24914,  0.24745,  0.24576],\n",
    "         [ 0.14949,  0.14849,  0.14748,  0.14647],\n",
    "         [-0.03105, -0.03084, -0.03063, -0.03043],\n",
    "         [-0.02082, -0.02068, -0.02054, -0.02040]]]\n",
    ")\n",
    "\n",
    "y = atten_multihead(query, key, value)\n",
    "dy = torch.randn(*y.shape)  \n",
    "\n",
    "y.backward(dy)\n",
    "query_grad = query.grad\n",
    "print(\"MultiHeadAttention error: \", rel_error(y_expected, y)) # error < 1e-5\n",
    "print(\"MultiHeadAttention error: \", rel_error(dy_expected, query_grad)) # error < 1e-5\n",
    "if rel_error(y_expected, y)> 1e-5 and rel_error(dy_expected, query_grad) > 1e-5:\n",
    "    print('There are some problems with your code, please check!')\n",
    "else:\n",
    "    print('The error is less than 1e-5. Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9be6374e-a968-4b56-a1e4-31d745330b62"
   },
   "source": [
    "## LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "524ff12e"
   },
   "source": [
    "我们在处理 CNN 时实现了 BatchNorm。BatchNorm 的一个问题是它依赖于完整的批次，当批次大小较小时可能会导致结果不佳。Ba 等人提出了 `LayerNormalization`，它考虑了这些问题，在序列到序列任务中已成为标准。在本节中，我们将实现 `LayerNormalization`。`LayerNormalization` 的另一个好处是，它依赖于序列的各个时间步骤或每个元素，因此可以并行化，并且测试时间的运行方式类似，因此在实现上更好。同样，您只需实现前向传播，而反向传播将由 Pytorch 自动求导来处理。在 `transformers.py` 中实现 `LayerNormalization` 类。可参考[LayerNorm](https://arxiv.org/abs/1607.06450)原理。\n",
    "\n",
    "\n",
    "可参考：\n",
    "1. [torch.mean](https://pytorch.org/docs/stable/generated/torch.mean.html#torch-mean)\n",
    "2. [torch.sqrt](https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch-sqrt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码填空4: 层归一化（10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdbc1bf9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, emb_dim: int, epsilon: float = 1e-10):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        该类实现了 Transformers 中线性层的层归一化。与 BatchNorm 不同，它估计每个批次中的每个元素的归一化统计量，因此不依赖于完整的批次。\n",
    "        输入形状类似 (N, K, M)，其中 N 是批次大小，K 是序列长度，M 是序列长度的嵌入。我们使用形状为 (N, K) 的均值和形状为 (N, K) 的标准差来归一化每个序列。\n",
    "\n",
    "        参数:\n",
    "            emb_dim: 表示嵌入维度的整数\n",
    "            epsilon: 浮点数值\n",
    "\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.beta = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        \"\"\"\n",
    "        层归一化层的前向传播的实现。\n",
    "\n",
    "        参数:\n",
    "            x: 形状为 (N, K, M) 或 (N, K) 的张量，其中 N 是批次大小，K 是序列长度，M 是嵌入维度\n",
    "\n",
    "        返回:\n",
    "            y: 在应用层归一化后的形状为 (N, K, M) 或 (N, K) 的张量\n",
    "\n",
    "        \"\"\"\n",
    "        ##########################################################################\n",
    "        # TODO: 实现层归一化层的前向传播。                                      #\n",
    "        # 计算输入的均值和标准差，并使用它们对输入进行归一化。                       #\n",
    "        # 进一步，使用 self.gamma 和 self.beta 来缩放和偏移这个归一化的输入。         #\n",
    "        # 不要使用 torch.std 来计算标准差。                                      #\n",
    "        ##########################################################################\n",
    "        mean = # TODO:计算输入的均值，在最后1个维度上计算的， 可参考torch.mean，注意keepdim的用法\n",
    "        var = # TODO：计算方差，在最后一个维度上计算,注意keepdim的用法\n",
    "        std = # TODO: 计算标准差\n",
    "        y = (x - mean) / std # 进行归一化\n",
    "        return self.gamma * y + self.beta # 使用self.gamma和self.beta来缩放和偏移归一化的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试代码4: 层归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdf95ed8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "K = 4\n",
    "norm = LayerNormalization(K)\n",
    "inp = torch.linspace(-0.4, 0.6, steps=N * K, requires_grad=True).reshape(N, K)\n",
    "\n",
    "inp.retain_grad()\n",
    "y = norm(inp)\n",
    "\n",
    "y_expected = torch.tensor(\n",
    "    [[-1.34164, -0.44721, 0.44721, 1.34164], [-1.34164, -0.44721, 0.44721, 1.34164]]\n",
    ")\n",
    "\n",
    "dy_expected = torch.tensor(\n",
    "    [[  5.70524,  -2.77289, -11.56993,   8.63758],\n",
    "        [  2.26242,  -4.44330,   2.09933,   0.08154]]\n",
    ")\n",
    "\n",
    "dy = torch.randn(*y.shape)\n",
    "y.backward(dy)\n",
    "inp_grad = inp.grad\n",
    "\n",
    "print(\"LayerNormalization error: \", rel_error(y_expected, y)) # error < 1e-5\n",
    "print(\"LayerNormalization grad error: \", rel_error(dy_expected, inp_grad)) # error < 1e-5\n",
    "if rel_error(y_expected, y)> 1e-5 and rel_error(dy_expected, inp_grad) > 1e-5:\n",
    "    print('There are some problems with your code, please check!')\n",
    "else:\n",
    "    print('The error is less than 1e-5. Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e86954f-f7df-4d7f-ab6c-2fe1dcb1f5b0"
   },
   "source": [
    "## FeedForward Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d49a145"
   },
   "source": [
    "接下来，我们将实现 `Feedforward` 块。这些在 Transformer 的编码器和解码器网络中都会使用，它们由堆叠的 MLP 和 ReLU 层组成。在总体架构中，`MultiHeadAttention` 的输出被输入到 `FeedForward` 块中。在 `transformers.py` 中实现 `FeedForwardBlock`，并执行以下单元格以检查你的实现。可参考：[nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, inp_dim: int, hidden_dim_feedforward: int):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Transformers 中 FeedForward 块的实现。我们通过两个堆叠的 MLP 和一个 ReLU 层传递输入。\n",
    "        前向传播具有以下架构:\n",
    "        \n",
    "        线性层 - ReLU - 线性层\n",
    "        \n",
    "        输入的形状将是 (N, K, M)，其中 N 是批量大小，K 是序列长度，M 是嵌入维度。\n",
    "        \n",
    "        参数:\n",
    "            inp_dim: 输入张量的嵌入维度的整数表示\n",
    "                     \n",
    "            hidden_dim_feedforward: feedforward 块的隐藏维度\n",
    "        \"\"\"\n",
    "        ##########################################################################\n",
    "        # 在这里初始化两个 MLP，第一个使用 inp_dim 作为输入维度，hidden_dim_feedforward 作为输出，\n",
    "        # 第二个使用 hidden_dim_feedforward 作为输入。使用 inp_dim 作为输出维度         \n",
    "        ##########################################################################\n",
    "        \n",
    "        self.linear1 = nn.Linear(inp_dim, hidden_dim_feedforward)\n",
    "        self.linear2 = nn.Linear(hidden_dim_feedforward, inp_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        nn.init.uniform_(self.linear1.weight, -torch.sqrt(torch.tensor(6. / (inp_dim + hidden_dim_feedforward))), torch.sqrt(torch.tensor(6. / (inp_dim + hidden_dim_feedforward))))\n",
    "        nn.init.uniform_(self.linear2.weight, -torch.sqrt(torch.tensor(6. / (hidden_dim_feedforward + inp_dim))), torch.sqrt(torch.tensor(6. / (hidden_dim_feedforward + inp_dim))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        FeedForward 块的前向传播的实现。\n",
    "\n",
    "        参数:\n",
    "            x: 形状为 (N, K, M) 的张量，是 MultiHeadAttention 的输出\n",
    "        返回:\n",
    "            y: 形状为 (N, K, M) 的张量\n",
    "            \n",
    "        两个 MLP 层执行前向传播。在第一个 MLP 后使用 ReLU 层，\n",
    "        在第二个 MLP 后不使用激活函数。\n",
    "        结构： linear1 -> relu -> linear2\n",
    "        \"\"\"\n",
    "        y = self.linear1(x)\n",
    "        y = self.relu(y)\n",
    "        y = self.linear2(y)\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09489f50"
   },
   "source": [
    "## Encoder 模块\n",
    "现在，如果你回顾一下原始论文《Attention is all you Need》，那么我们几乎完成了 Transformer 的构建模块。剩下的是：\n",
    "\n",
    "- 将构建模块封装成编码器块\n",
    "- 将构建模块封装成解码器块\n",
    "- 处理输入数据的预处理和位置编码。\n",
    "\n",
    "\n",
    "我们首先来实现编码器块和解码器块。位置编码是一个非可学习的嵌入，我们可以将其视为 DataLoader 中的预处理步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1364689-db62-44f4-85ca-ed005a357f54"
   },
   "source": [
    "编码器块的输入是三个张量。我们假设我们有这三个张量，查询（query）、键（key）和值（value）。运行下面的单元格来检查你的 EncoderBlock 实现。你应该期望错误在 1e-5 以下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码填空5: Encoder模块（10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b9fa5fea",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_heads: int, emb_dim: int, feedforward_dim: int, dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        这个类实现了 Transformer 模型的编码器块。原始论文中使用了 6 个这样的块串行地训练了最终模型。\n",
    "        在这里，我们首先使用已经实现的构建块初始化所需的层，然后最终编写前向传播过程，其中包括残差连接和 dropout。\n",
    "\n",
    "        如论文中的图1所示，注意力就是你所需要的，\n",
    "        编码器由四个组件组成:\n",
    "\n",
    "        1. MultiHead Attention\n",
    "        2. FeedForward 层\n",
    "        3. 多头注意力和前馈层之后的残差连接\n",
    "        4. LayerNorm\n",
    "\n",
    "        架构如下:\n",
    "\n",
    "       输入 - 多头注意力 - 输出1 - 层规范化(输出1 + 输入) - dropout - 输出2 \\ \n",
    "        - 前馈 - 输出3 - 层规范化(输出3 + 输出2) - dropout - 输出\n",
    "\n",
    "        这里，输入是多头注意力的输入，形状为 (N, K, M)，输出1、输出2 和输出3 是相应层的输出，\n",
    "        我们将这些输出添加到它们各自的输入中以实现残差连接。\n",
    "\n",
    "        参数:\n",
    "            num_heads: int 值，指定编码器中 MultiHeadAttention 块的头数\n",
    "            emb_dim: int 值，指定输入序列的嵌入维度\n",
    "            feedforward_dim: int 值，指定 Transformer 的 FeedForward 层中的隐藏单元数\n",
    "            dropout: float 值，指定 dropout 值\n",
    "        \"\"\"\n",
    "\n",
    "        if emb_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"\"\"嵌入维度 emb_dim = {emb_dim} 不能被\n",
    "                             num_heads = {num_heads} 整除。请选择一个\n",
    "                             合适的值。\"\"\"\n",
    "            )\n",
    "\n",
    "        ##########################################################################\n",
    "        # 初始化以下层:                                                    #\n",
    "        # 1. 一个 MultiHead Attention 块，使用 num_heads 作为头数和             #\n",
    "        #    emb_dim 作为输入维度。您还应该能够计算 MultiheadHead 注意力的输出  #\n",
    "        #    维度，给定 num_heads 和 emb_dim。                                    #\n",
    "        #    提示: 使用的逻辑是在 MultiHead Attention 块内连接每个               #\n",
    "        #    SingleHeadAttention 的输出，并选择输出维度，使得连接后的张量和    #\n",
    "        #    输入张量具有相同的嵌入维度。                                       #\n",
    "        #                                                                        #\n",
    "        # 2. 两个输入维度等于 emb_dim 的 LayerNorm 层                          #\n",
    "        # 3. 一个以 emb_dim 为输入，feedforward_dim 为隐藏单元的 FeedForward 块  #\n",
    "        # 4. 一个具有给定 dropout 参数的 Dropout 层                             #\n",
    "        ##########################################################################\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads, emb_dim, int(emb_dim // num_heads))\n",
    "        self.layer_norm1 = LayerNormalization(emb_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.feedforward = FeedForwardBlock(emb_dim, feedforward_dim)\n",
    "        self.layer_norm2 = LayerNormalization(emb_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\"\n",
    "        Transformer 模型的 EncoderBlock 的前向传播的实现。\n",
    "        参数:\n",
    "            x: 形状为 (N, K, M) 的张量，作为输入序列\n",
    "        返回:\n",
    "            y: 形状为 (N, K, M) 的张量，作为前向传播的输出\n",
    "         \n",
    "       架构如下:\n",
    "\n",
    "       输入 - 多头注意力 - 输出1 - 层规范化(输出1 + 输入) - dropout - 输出2 \\ \n",
    "        - 前馈 - 输出3 - 层规范化(输出3 + 输出2) - dropout - 输出\n",
    "\n",
    "        \"\"\"\n",
    "        y = None\n",
    "        ##########################################################################\n",
    "        # TODO: 使用在 init 函数中初始化的层完成前向传播。由于 Multihead Attention #\n",
    "        # 需要3个输入，因此将同一个输入x用三次作为输入。\n",
    "        ##########################################################################\n",
    "        out1 = # TODO\n",
    "        out2 = # TODO\n",
    "        out3 = # TODO\n",
    "        y = # TODO\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试代码5：Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76e8ccb0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "reset_seed(0)\n",
    "N = 2\n",
    "num_heads = 2\n",
    "emb_dim = K = 4\n",
    "feedforward_dim = 8\n",
    "M = inp_emb_size = 4\n",
    "out_emb_size = 8\n",
    "dropout = 0.2\n",
    "\n",
    "enc_seq_inp = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")\n",
    "\n",
    "enc_block = EncoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
    "\n",
    "\n",
    "for k, v in enc_block.named_parameters():\n",
    "#     print(k, v.shape) \n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "encoder_out1_expected = torch.tensor(\n",
    "    [[[ 0.00000, -0.31357,  0.69126,  0.00000],\n",
    "         [ 0.42630, -0.25859,  0.72412,  3.87013],\n",
    "         [ 0.00000, -0.31357,  0.69126,  3.89884],\n",
    "         [ 0.47986, -0.30568,  0.69082,  3.90563]],\n",
    "\n",
    "        [[ 0.00000, -0.31641,  0.69000,  3.89921],\n",
    "         [ 0.47986, -0.30568,  0.69082,  3.90563],\n",
    "         [ 0.47986, -0.30568,  0.69082,  3.90563],\n",
    "         [ 0.51781, -0.30853,  0.71598,  3.85171]]]\n",
    ")\n",
    "encoder_out1 = enc_block(enc_seq_inp)\n",
    "print(\"EncoderBlock error 1: \", rel_error(encoder_out1, encoder_out1_expected))\n",
    "\n",
    "\n",
    "\n",
    "N = 2\n",
    "num_heads = 1\n",
    "emb_dim = K = 4\n",
    "feedforward_dim = 8\n",
    "M = inp_emb_size = 4\n",
    "out_emb_size = 8\n",
    "dropout = 0.2\n",
    "\n",
    "enc_seq_inp = torch.linspace(-0.4, 0.6, steps=N * K * M, requires_grad=True).reshape(\n",
    "    N, K, M\n",
    ")  \n",
    "\n",
    "enc_block = EncoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
    "\n",
    "for k, v in enc_block.named_parameters():\n",
    "    # print(k, v.shape) \n",
    "    v.data.copy_(torch.linspace(-1.4, 1.3, steps=v.numel()).reshape(*v.shape))\n",
    "\n",
    "encoder_out2_expected = torch.tensor(\n",
    "    [[[ 0.42630, -0.00000,  0.72412,  3.87013],\n",
    "         [ 0.49614, -0.31357,  0.00000,  3.89884],\n",
    "         [ 0.47986, -0.30568,  0.69082,  0.00000],\n",
    "         [ 0.51654, -0.32455,  0.69035,  3.89216]],\n",
    "\n",
    "        [[ 0.47986, -0.30568,  0.69082,  0.00000],\n",
    "         [ 0.49614, -0.31357,  0.69126,  3.89884],\n",
    "         [ 0.00000, -0.30354,  0.76272,  3.75311],\n",
    "         [ 0.49614, -0.31357,  0.69126,  3.89884]]]\n",
    ")\n",
    "encoder_out2 = enc_block(enc_seq_inp)\n",
    "print(\"EncoderBlock error 2: \", rel_error(encoder_out2, encoder_out2_expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b2616ef-5934-4a50-8f51-642ef635e2cb"
   },
   "source": [
    "## Decoder Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19e2d5d7-6918-4ea8-a369-0d53304ff1a7"
   },
   "source": [
    "现在，我们将看一下解码器的实现。对于更复杂的任务，如序列到序列的任务，我们需要一个解码器网络，它可以将编码器的输出转换为目标序列。这种架构在诸如语言翻译之类的任务中非常重要，其中我们有一个序列作为输入和一个序列作为输出。这个解码器从编码器获取输入和先前生成的值来生成下一个值。在训练过程中，我们在输入上使用掩码，以便解码器网络不能预先查看未来，在推理过程中，我们顺序处理数据。\n",
    "\n",
    "在转到实现解码器块之前，我们应该注意\"Masked MultiHead Attention\"，实际上防止了解码器预测未来。让我们通过一个例子来理解这个。我们有一个表达式 `BOS POSITIVE 01 add POSITIVE 00 EOS`，即 `1+0`，它的输出是 `BOS POSITIVE 01 EOS`，即 `+1`。让我们关注输出序列。这是一个长度为5的序列（经过我们的预处理代码），将会转换成维度为$5\\times128$的 *key*、*query* 和 *value* 矩阵，其中128是Transformer的嵌入维度。现在，在训练过程中，我们将这些向量输入到 `self_attention_no_loop_batch` 中而不使用掩码。它将计算 *query* 和 *key* 之间的点积，生成一个 $5\\times5$ 的矩阵，其中第一行（形状为 $1\\times5$）告诉我们单词 `EOS` 与 `EOS`、`POSITIVE`、`0`、`1` 和 `EOS` 之间的相关程度。这意味着它将使用所有这些标记的权重来学习最终要预测的序列。当我们训练模型时，这是可以接受的，但当我们进行推理时会发生什么呢？我们从一个全新的表达式开始，将这个表达式输入编码器，但这次我们只有解码器的第一个起始标记 `EOS`，我们不知道序列中的其他标记。因此，解决这个问题的方法是在函数 `self_attention_no_loop_batch` 中为解码器部分掩码权重。这种掩码应该防止解码器访问未来或下一个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_subsequent_mask(seq):\n",
    "    \"\"\"\n",
    "    解码器自注意力掩码的实现。在训练模型时，这将用于屏蔽目标序列。这里的输入形状是 (N, K)，其中 N 是批量大小，K 是序列长度。\n",
    "\n",
    "    参数:\n",
    "        seq: 形状为 (N, K) 的张量，其中 N 是批量大小，K 是序列长度\n",
    "    返回:\n",
    "        mask: 形状为 (N, K, K) 的张量，其中 N 是批量大小，K 是序列长度\n",
    "\n",
    "    给定长度为 K 的序列，我们希望在函数 `self_attention_no_loop_batch` 中掩码权重，\n",
    "    以防止解码器预测未来。\n",
    "    \"\"\"\n",
    "    ###############################################################################\n",
    "    # 这个函数构造了 Transformer 解码器部分的掩码。                         #\n",
    "    # 要实现这一点，对于批量（N）中的每个序列（长度为 K），返回一个布尔矩阵，       #\n",
    "    # 在应用掩码的地方为 True，不应用掩码的地方为 False。                         #                                                                           #\n",
    "    ###############################################################################\n",
    "    N, K = seq.shape\n",
    "    ones_matrix = torch.ones((N, K, K), device=seq.device, dtype=torch.bool)\n",
    "    mask = torch.triu(ones_matrix, diagonal=1)\n",
    "\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_heads: int, emb_dim: int, feedforward_dim: int, dropout: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if emb_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"\"\"The value emb_dim = {emb_dim} is not divisible\n",
    "                             by num_heads = {num_heads}. Please select an\n",
    "                             appropriate value.\"\"\"\n",
    "            )\n",
    "\n",
    "        \"\"\"\n",
    "        该函数实现了Transformer模型的解码器块。\n",
    "    \n",
    "        args:\n",
    "            num_heads: 表示头数的整数值\n",
    "\n",
    "            emb_dim: 表示嵌入维度的整数值\n",
    "\n",
    "            feedforward_dim: 表示前馈模型中的隐藏层的整数值\n",
    "\n",
    "            dropout: 表示丢弃概率的浮点数值\n",
    "        \"\"\"\n",
    "        ##########################################################################\n",
    "        # 初始化以下层:                                                   #\n",
    "        # 1. 两个具有 num_heads 个头数和 emb_dim 作为嵌入维度的MultiheadAttention 层。#\n",
    "        #     与编码器一样，你应该能够确定两个MultiHeadAttention的输出维度。            #\n",
    "        # 2. 一个FeedForward块，它将emb_dim作为输入维度，feedforward_dim作为隐藏层的维度 #\n",
    "        # 3. 在每个块之后添加LayerNormalization层                                      #\n",
    "        # 4. 在每个块之后添加丢弃层                                                    #\n",
    "        ##########################################################################\n",
    "\n",
    "        self.attention_self = MultiHeadAttention(num_heads, emb_dim, int(emb_dim/num_heads))\n",
    "        self.attention_cross = MultiHeadAttention(num_heads, emb_dim, int(emb_dim/num_heads))\n",
    "\n",
    "        self.feed_forward = FeedForwardBlock(emb_dim, feedforward_dim)\n",
    "\n",
    "        self.norm1 = LayerNormalization(emb_dim)\n",
    "        self.norm2 = LayerNormalization(emb_dim)\n",
    "        self.norm3 = LayerNormalization(emb_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, dec_inp: Tensor, enc_inp: Tensor, mask: Tensor = None\n",
    "    ) -> Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        args:\n",
    "            dec_inp: 形状为 (N, K, M) 的张量\n",
    "            enc_inp: 形状为 (N, K, M) 的张量\n",
    "            mask: 形状为 (N, K, K) 的张量\n",
    "\n",
    "        此函数将处理解码器块的前向传递。它将输入 enc_inp 作为编码器输出，\n",
    "        以及一个张量 dec_inp 作为目标序列，训练时目标序列将向后移动一个位置，\n",
    "        推理时为一个初始标记 \"BOS\"。\n",
    "        \"\"\"\n",
    "        ##########################################################################\n",
    "        # 使用init函数中初始化的层，实现解码器块的前向传递。将 dec_inp 传递给  #\n",
    "        # self.attention_self 层。这一层负责解码器输入的自我交互。                # \n",
    "        # 架构如下:\n",
    "        \n",
    "        # 输入 - 带有mask的多头注意力 - 输出1 - 层归一化(输入 + 输出1) - \\\n",
    "        # dp层 - (输出2和编码器输出) - 多头注意力 - 输出3 - \\\n",
    "        # 层归一化(输出3 + 输出2) - dp层 - 输出4 - 前馈网络 - 输出5 - \\\n",
    "        # 层归一化(输出5 + 输出4) - dp层 - 输出\n",
    "        \n",
    "        # 这里，输出1、输出2、输出3、输出4、输出5是各层对应的输出，编码器输出是编码器的输出，\n",
    "        # 我们将这些输出添加到它们各自的输入中以实现残差连接。\n",
    "        \n",
    "        ##########################################################################\n",
    "        out1 = self.attention_self(dec_inp, dec_inp, dec_inp, mask) # (N, K2, M)\n",
    "        out2 = self.norm1(out1 + dec_inp) # (N, K2, M)\n",
    "        out2 = self.dropout(out2) # (N, K2, M)\n",
    "        out3 = self.attention_cross(out2, enc_inp, enc_inp) # query: out2, key&values: enc_inp\n",
    "                                                            # out3: (N, K2, M)\n",
    "        out4 = self.norm2(out3 + out2) # (N, K2, M)\n",
    "        out4 = self.dropout(out4) # (N, K2, M)\n",
    "        out5 = self.feed_forward(out4) # (N, K2, M)\n",
    "        out = self.norm3(out5 + out4) # (N, K2, M)\n",
    "        y = self.dropout(out) # (N, K2, M)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建Transformer结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer的结构由N个encoder和decoder结构组成，以下代码将使用nn.ModuleList将N个编码器和解码器串联起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        emb_dim: int,\n",
    "        feedforward_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        该类封装了使用多个EncoderBlock层的最终编码器的实现。\n",
    "\n",
    "        args:\n",
    "            num_heads: 表示EncoderBlock中要使用的头数的整数值\n",
    "            emb_dim: 表示Transformer模型的嵌入维度的整数值\n",
    "            feedforward_dim: 表示前馈块的隐藏层维度的整数值\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src_seq: Tensor):\n",
    "        for _layer in self.layers:\n",
    "            src_seq = _layer(src_seq)\n",
    "\n",
    "        return src_seq\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        emb_dim: int,\n",
    "        feedforward_dim: int,\n",
    "        num_layers: int,\n",
    "        dropout: float,\n",
    "        vocab_len: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        解码器从编码器接收输入和目标序列，生成输出的最终序列。我们首先将输入通过堆叠的DecoderBlock，\n",
    "        然后将输出投影到vocab_len，这是获取实际序列所需的。\n",
    "\n",
    "        args:\n",
    "            num_heads: 表示Transformer中MultiheadAttention的头数的整数\n",
    "            emb_dim: 表示序列的嵌入维度的整数\n",
    "            feedforward_dim: 表示前馈块中隐藏层的维度的整数\n",
    "            num_layers: 表示解码器中DecoderBlock的数量的整数\n",
    "            dropout: 表示每个DecoderBlock中的丢弃率的浮点数\n",
    "            vocab_len: 词汇表的长度\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(num_heads, emb_dim, feedforward_dim, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.proj_to_vocab = nn.Linear(emb_dim, vocab_len)\n",
    "        a = (6 / (emb_dim + vocab_len)) ** 0.5\n",
    "        nn.init.uniform_(self.proj_to_vocab.weight, -a, a)\n",
    "\n",
    "    def forward(self, target_seq: Tensor, enc_out: Tensor, mask: Tensor):\n",
    "\n",
    "        out = target_seq.clone()\n",
    "        for _layer in self.layers:\n",
    "            out = _layer(out, enc_out, mask)\n",
    "        out = self.proj_to_vocab(out)\n",
    "        return out\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        emb_dim: int,\n",
    "        feedforward_dim: int,\n",
    "        dropout: float,\n",
    "        num_enc_layers: int,\n",
    "        num_dec_layers: int,\n",
    "        vocab_len: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        该类实现了带有编码器和解码器的Transformer模型。模型的输入是形状为(N, K)的张量，\n",
    "        输出是形状为(N*O, V)的张量。这里，N是批量大小，K是输入序列长度，O是输出序列长度，\n",
    "        V是词汇表大小。输入通过共享的nn.Embedding层传递，然后加上输入位置编码。\n",
    "        类似地，目标通过相同的nn.Embedding层传递，然后加上目标位置编码。唯一的区别是，\n",
    "        我们在目标中取倒数第二个值。总结的输入（请查看代码了解详情）然后通过编码器和解码器\n",
    "        块以获得最终输出。\n",
    "        args:\n",
    "            num_heads: 表示Encoder和Decoder中要使用的头数的整数\n",
    "            emb_dim: 表示Transformer的嵌入维度的整数\n",
    "            dim_feedforward: 表示Encoder和Decoder中隐藏层的数量的整数\n",
    "            dropout: 表示丢弃层的概率的浮点数\n",
    "            num_enc_layers: 表示编码器块的数量的整数\n",
    "            num_dec_layers: 表示解码器块的数量的整数\n",
    "\n",
    "        \"\"\"\n",
    "        self.emb_layer = None\n",
    "        ##########################################################################\n",
    "        # 初始化一个Embedding层，将vocab_len映射到emb_dim。这是我们模型的第一个输入，\n",
    "        # 并将此输入转换为整个模型中保持不变的emb_dim。请使用self.emb_layer作为此层的名称。 #\n",
    "        ##########################################################################\n",
    "        self.emb_layer = nn.Embedding(vocab_len, emb_dim)\n",
    "        self.encoder = Encoder(\n",
    "            num_heads, emb_dim, feedforward_dim, num_enc_layers, dropout\n",
    "        )\n",
    "        self.decoder = Decoder(\n",
    "            num_heads,\n",
    "            emb_dim,\n",
    "            feedforward_dim,\n",
    "            num_dec_layers,\n",
    "            dropout,\n",
    "            vocab_len,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self, ques_b: Tensor, ques_pos: Tensor, ans_b: Tensor, ans_pos: Tensor\n",
    "    ) -> Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        实现Transformer的前向传递。\n",
    "\n",
    "        args:\n",
    "            ques_b: 形状为(N, K)的张量，包含算术表达式的输入序列\n",
    "            ques_pos: 形状为(N, K, M)的张量，包含输入序列的位置编码\n",
    "            ans_b: 形状为(N, K)的张量，包含算术表达式的目标序列\n",
    "            ans_pos: 形状为(N, K, M)的张量，包含目标序列的位置编码\n",
    "\n",
    "        返回:\n",
    "            dec_out: 形状为(N*O, M)的张量，其中O是目标序列的大小。\n",
    "        \"\"\"\n",
    "        q_emb = self.emb_layer(ques_b)\n",
    "        a_emb = self.emb_layer(ans_b)\n",
    "        q_emb_inp = q_emb + ques_pos\n",
    "        a_emb_inp = a_emb[:, :-1] + ans_pos[:, :-1]\n",
    "        dec_out = None\n",
    "        ##########################################################################\n",
    "        # 此部分包括编写完整Transformer的前向部分。首先，将q_emb_inp传递给编码器，\n",
    "        # 这将是解码器输入之一。除了编码器输出外，还应构建一个适当的掩码，使用get_subsequent_mask。\n",
    "        # 最后，将a_emb_inp、编码器输出和掩码传递给解码器。任务是屏蔽目标(a_emb_inp)的值。  #\n",
    "        # 掩码形状将取决于张量ans_b                                      #\n",
    "        ##########################################################################\n",
    "        enc_out = self.encoder(q_emb_inp)\n",
    "        mask = get_subsequent_mask(ans_b[:,:-1])\n",
    "        dec_out = self.decoder(a_emb_inp, enc_out, mask)\n",
    "        dec_out = dec_out.reshape(-1, dec_out.shape[2])\n",
    "\n",
    "        return dec_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20a41e64-4de5-4289-be47-3e7282e88f35"
   },
   "source": [
    "# 第三部分：数据加载器\n",
    "\n",
    "在这一部分中，我们将创建用于训练Transformer模型的最终数据加载器。这将包括两件事情：\n",
    "\n",
    "- 实现位置编码（Positional Encoding）\n",
    "- 使用我们在第一部分创建的`prepocess_input_sequence`函数创建一个数据加载器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1249f651-4de3-4dd7-aa85-7d8bfb3e1e4f"
   },
   "source": [
    "让我们开始实现输入的位置编码。位置编码使得Transformer在处理序列时能够感知位置信息。这些位置编码通常被添加到输入中，因此它们的形状应该与输入相同。由于位置编码不可学习，在训练过程中它们保持不变。因此，我们可以将其视为对输入进行的预处理步骤。我们的策略是实现位置编码函数，并在稍后创建玩具数据集的DataLoader时使用它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c8ddbbc-1631-4781-849d-3fd27a3b0619"
   },
   "source": [
    "### Sinusoid positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e3db3a0"
   },
   "source": [
    "对于简单位置编码的一个主要缺点，那就是如果序列长度变大，两个连续位置编码之间的差异会变得越来越小，从而失去了位置感知的目的，因为连续位置之间的差异非常小。另一个问题是，对于每个位置，我们都沿着嵌入维度进行了复制，引入了冗余，这可能不会帮助网络学到任何新知识。可以使用不同的技巧来创建一个位置编码来解决这些问题。\n",
    "\n",
    "让我们看一下更成熟的位置编码版本，它使用正弦和余弦函数的组合，也称为正弦波。这也是原始Transformer论文中使用的位置编码。对于序列（长度为 K）中的每个元素，具有位置 $p$ 和嵌入（维度 M）位置 $i$，我们可以将位置编码定义为：\n",
    "\n",
    "$$PE_{(p, 2i)} = \\sin\\left(\\frac{p}{10000^a}\\right)$$\n",
    "$$PE_{(p, 2i+1)} = \\cos\\left(\\frac{p}{10000^a}\\right)$$\n",
    "\n",
    "$$\\text{其中 }a = \\left\\lfloor{\\frac{2i}{M}}\\right\\rfloor \\text{，而 M 是Transformer的嵌入维度}$$\n",
    "\n",
    "在这里，$p$ 对于序列中的位置保持不变，并且我们沿着嵌入维度交替分配正弦和余弦。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:21:42.057937Z",
     "iopub.status.busy": "2024-04-24T07:21:42.057552Z",
     "iopub.status.idle": "2024-04-24T07:21:42.066006Z",
     "shell.execute_reply": "2024-04-24T07:21:42.064799Z",
     "shell.execute_reply.started": "2024-04-24T07:21:42.057905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def position_encoding_sinusoid(K: int, M: int) -> Tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    正弦位置编码的实现。\n",
    "\n",
    "    参数:\n",
    "        K: 表示序列长度的整数\n",
    "        M: 表示序列的嵌入维度的整数\n",
    "\n",
    "    返回:\n",
    "        y: 形状为 (1, K, M) 的张量\n",
    "    \"\"\"\n",
    "    ##############################################################################\n",
    "    # 给定输入序列长度 K 和嵌入维度 M，构造形状为 (K, M) 的张量，其中沿着     #\n",
    "    # 维度的值遵循上面给出的方程。请注意在嵌入维度 M 上交替使用正弦和余弦。         #\n",
    "    ##############################################################################\n",
    "    y = torch.zeros(1, K, M)\n",
    "    temp = torch.arange(M)\n",
    "    temp = torch.div(temp, M, rounding_mode=\"floor\").reshape(1, -1) #(1, M)\n",
    "    p = torch.arange(K).unsqueeze(dim=-1)\n",
    "    \"\"\"\n",
    "    temp:我们得到的是从 0 到 1 之间的均匀间隔的值。\n",
    "    接下来，我们创建一个形状为 (K, 1) 的张量 num，其中包含从 0 到 K-1 的整数，并在最后一个维度上增加一个维度。\n",
    "    \"\"\"\n",
    "    y[0, :, 0::2] = torch.sin(p / torch.pow(10000, temp[0, 0::2]))\n",
    "    y[0, :, 1::2] = torch.cos(p / torch.pow(10000, temp[0, 1::2]))\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于Transformer在序列到序列任务上表现非常好，我们将在一个算术操作的玩具任务上实现它。我们将使用Transformer模型来执行两个整数的加法和减法，其中整数的绝对值最多为50。一个简单的例子是使用Transformer模型执行 `-5 + 2` 的计算，并得到正确的结果 `-3`。由于解决这个问题可以有多种方法，我们将看到如何将其作为序列到序列问题提出，并使用Transformer模型解决它。\n",
    "让我们先看一下数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:20:48.186858Z",
     "iopub.status.busy": "2024-04-24T07:20:48.186367Z",
     "iopub.status.idle": "2024-04-24T07:20:48.216201Z",
     "shell.execute_reply": "2024-04-24T07:20:48.215416Z",
     "shell.execute_reply.started": "2024-04-24T07:20:48.186825Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data_path = '/kaggle/input/hw-transformer-addsub/train_data.json'\n",
    "val_data_path = '/kaggle/input/hw-transformer-addsub/val_data.json'\n",
    "test_data_path = '/kaggle/input/hw-transformer-addsub/test_data.json'\n",
    "\n",
    "train_data = json.load(open(train_data_path))\n",
    "val_data = json.load(open(val_data_path))\n",
    "test_data = json.load(open(test_data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:20:49.947731Z",
     "iopub.status.busy": "2024-04-24T07:20:49.946911Z",
     "iopub.status.idle": "2024-04-24T07:20:49.952938Z",
     "shell.execute_reply": "2024-04-24T07:20:49.952062Z",
     "shell.execute_reply.started": "2024-04-24T07:20:49.947698Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_examples = 4\n",
    "for q, a in zip(\n",
    "    train_data[\"inp_expression\"][:num_examples], \n",
    "    train_data[\"out_expression\"][:num_examples]\n",
    "    ):\n",
    "  print(\"Expression: \" + q + \" Output: \" + a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这些示例代表什么意思：\n",
    "\n",
    "让我们首先看一下这里的第一个和第三个示例，并理解它们代表了什么：\n",
    "\n",
    "- 表达式：`BOS NEGATIVE 30 subtract NEGATIVE 34 EOS` 输出：`BOS POSITIVE 04 EOS`：这里的表达式是 $(-30) - (-34)$。这里符号 `+` 有两个含义：一个是表示数字的符号，另一个是两个整数之间的加法操作。为了简化神经网络的问题，我们用不同的文本标记表示它们。$(-30) - (-34)$ 的输出是 $+4$。这里的 `BOS` 和 `EOS` 分别指示序列的开头和结尾。\n",
    "- 同样，第二个表达式，`BOS NEGATIVE 34 add NEGATIVE 15 EOS` 输出：`BOS NEGATIVE 49 EOS` 意味着我们正在进行计算 $(-34) + (-15)$。与上面类似，这里的符号 `-` 表示两个东西：第一个是整数的符号，第二个是两个整数之间的操作。同样，我们用不同的标记表示，以简化神经网络的问题。这里的输出是 -49。这里的 `BOS` 和 `EOS` 分别指示序列的开头和结尾。\n",
    "\n",
    "现在我们对数据有了一定的了解，让我们开始对数据进行预处理，因为神经网络无法理解字符串，我们需要将它们表示为数字。\n",
    "\n",
    "## 预处理数据\n",
    "我们需要将原始输入序列转换为可以用神经网络处理的格式。\n",
    "具体来说，我们需要将一个人类可读的字符串（例如 `BOS NEGATIVE 30 subtract NEGATIVE 34 EOS`）转换为一系列**标记**，其中每个标记将是一个整数。\n",
    "将输入字符串转换为一系列标记的过程称为**标记化**。\n",
    "\n",
    "在我们可以对任何特定序列进行标记化之前，我们首先需要构建一个**词汇表**；这是一个包含数据集中所有标记的详尽列表，并且从每个标记到一个唯一整数值的映射。\n",
    "在我们的例子中，我们的词汇表将包含16个元素：一个条目表示每个数字 `0` 到 `9`，两个标记用于表示数字的符号（`POSITIVE` 和 `NEGATIVE`），两个标记表示加法和减法运算（`add` 和 `subtract`），最后两个特殊标记表示序列的开头和结尾（`BOS`，`EOS`）。\n",
    "\n",
    "我们通常使用一对数据结构来表示词汇表。\n",
    "第一个是所有字符串标记的列表（下面的 `vocab`），使得 `vocab[i] = s` 意味着字符串 `s` 被分配了整数值 `i`。这使我们能够查找与任何数值索引 `i` 关联的字符串。\n",
    "我们还需要一个数据结构，使我们能够反向映射：给定一个字符串 `s`，找到它分配的索引 `i`。通常，这表示为一个哈希映射（Python中的 `dict` 对象），其键是字符串，其值是分配给这些字符串的索引。\n",
    "您将实现函数 `generate_token_dict`，该函数输入列表 `vocab` 并返回给定此映射的字典 `convert_str_to_token`。\n",
    "\n",
    "一旦您构建了词汇表，然后您可以实现函数 `preprocess_input_sequence`，它使用词汇表数据结构将输入字符串转换为整数标记列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:21:13.318792Z",
     "iopub.status.busy": "2024-04-24T07:21:13.318391Z",
     "iopub.status.idle": "2024-04-24T07:21:13.323864Z",
     "shell.execute_reply": "2024-04-24T07:21:13.322837Z",
     "shell.execute_reply.started": "2024-04-24T07:21:13.318759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = [\"POSITIVE\", \"NEGATIVE\", \"add\", \"subtract\", \"BOS\", \"EOS\"]\n",
    "vocab = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"] + SPECIAL_TOKENS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:21:29.406130Z",
     "iopub.status.busy": "2024-04-24T07:21:29.405331Z",
     "iopub.status.idle": "2024-04-24T07:21:29.411275Z",
     "shell.execute_reply": "2024-04-24T07:21:29.410393Z",
     "shell.execute_reply.started": "2024-04-24T07:21:29.406098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_token_dict(vocab):\n",
    "    \"\"\"\n",
    "    该函数从词汇表中的元素创建一个哈希映射，将其映射到唯一的正整数值。\n",
    "\n",
    "    参数:\n",
    "        - vocab: 这是一个包含词汇表中所有项目的字符串的一维列表\n",
    "\n",
    "    返回值:\n",
    "        - token_dict: 一个Python字典，键是词汇表中的字符串项，值是唯一的整数值\n",
    "    \"\"\"\n",
    "\n",
    "    ##############################################################################                             #\n",
    "    # 使用此函数为词汇表列表中的每个元素分配一个唯一的整数元素。\n",
    "    #  将词汇表中的第一个元素映射为0，将词汇表中的最后一个元素映射为len(vocab)，\n",
    "    #  并将中间的元素映射为连续的数字。\n",
    "    ##############################################################################\n",
    "    token_dict = {}\n",
    "    for idx, item in enumerate(vocab):\n",
    "      token_dict[item] = idx\n",
    "    return token_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:21:16.494740Z",
     "iopub.status.busy": "2024-04-24T07:21:16.494330Z",
     "iopub.status.idle": "2024-04-24T07:21:16.501894Z",
     "shell.execute_reply": "2024-04-24T07:21:16.500858Z",
     "shell.execute_reply.started": "2024-04-24T07:21:16.494707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepocess_input_sequence(\n",
    "    input_str: str, token_dict: dict, spc_tokens: list\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    该函数的目标是将输入字符串转换为一系列正整数，以便我们能够进一步使用神经网络处理该字符串。我们将使用上一个函数中生成的字典将字符串中的元素映射到唯一的值。\n",
    "    请注意，我们为输入序列中的每个整数分配一个值。例如，对于输入序列中存在的数字 \"33\"，您应该将其拆分为数字列表，['0', '3']，并将其分配给token_dict中相应的值。\n",
    "\n",
    "    参数:\n",
    "    - input_str: 输入数据中的单个字符串\n",
    "               例如: \"BOS POSITIVE 0333 add POSITIVE 0696 EOS\"\n",
    "    - token_dict: 令牌字典，其键为字符串中的元素，值为唯一的正整数。这是使用 generate_token_dict 函数生成的\n",
    "    - spc_tokens: 除了数字以外的特殊令牌。\n",
    "    返回值:\n",
    "    - out_tokens: 与输入字符串对应的整数列表\n",
    "\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    ##############################################################################\n",
    "    # 对于输入序列中的每个数字，将其拆分为数字列表，并使用该数字列表从token_dict中分配适当的值。\n",
    "    # 对于输入字符串中存在的特殊令牌，为完整的令牌分配适当的值。\n",
    "    ##############################################################################\n",
    "    for item in input_str.split():\n",
    "      if item in spc_tokens:\n",
    "        out.append(token_dict[item])\n",
    "\n",
    "      else:\n",
    "        for digit in item:\n",
    "          out.append(token_dict[digit])\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:21:31.337765Z",
     "iopub.status.busy": "2024-04-24T07:21:31.337380Z",
     "iopub.status.idle": "2024-04-24T07:21:31.447802Z",
     "shell.execute_reply": "2024-04-24T07:21:31.446816Z",
     "shell.execute_reply.started": "2024-04-24T07:21:31.337733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "convert_str_to_tokens = generate_token_dict(vocab)\n",
    "\n",
    "ex1_in = \"BOS POSITIVE 0333 add POSITIVE 0696 EOS\"\n",
    "ex2_in = \"BOS POSITIVE 0673 add POSITIVE 0675 EOS\"\n",
    "ex3_in = \"BOS NEGATIVE 0286 subtract NEGATIVE 0044 EOS\"\n",
    "ex4_in = \"BOS NEGATIVE 0420 add POSITIVE 0342 EOS\"\n",
    "\n",
    "ex1_out = \"BOS POSITIVE 1029 EOS\"\n",
    "ex2_out = \"BOS POSITIVE 1348 EOS\"\n",
    "ex3_out = \"BOS NEGATIVE 0242 EOS\"\n",
    "ex4_out = \"BOS NEGATIVE 0078 EOS\"\n",
    "\n",
    "ex1_inp_preprocessed = torch.tensor(\n",
    "    prepocess_input_sequence(ex1_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex2_inp_preprocessed = torch.tensor(\n",
    "    prepocess_input_sequence(ex2_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex3_inp_preprocessed = torch.tensor(\n",
    "    prepocess_input_sequence(ex3_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex4_inp_preprocessed = torch.tensor(\n",
    "    prepocess_input_sequence(ex4_in, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "\n",
    "ex1_processed_expected = torch.tensor([14, 10, 0, 3, 3, 3, 12, 10, 0, 6, 9, 6, 15])\n",
    "ex2_processed_expected = torch.tensor([14, 10, 0, 6, 7, 3, 12, 10, 0, 6, 7, 5, 15])\n",
    "ex3_processed_expected = torch.tensor([14, 11, 0, 2, 8, 6, 13, 11, 0, 0, 4, 4, 15])\n",
    "ex4_processed_expected = torch.tensor([14, 11, 0, 4, 2, 0, 12, 10, 0, 3, 4, 2, 15])\n",
    "\n",
    "ex1_out = torch.tensor(\n",
    "    prepocess_input_sequence(ex1_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex2_out = torch.tensor(\n",
    "    prepocess_input_sequence(ex2_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex3_out = torch.tensor(\n",
    "    prepocess_input_sequence(ex3_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "ex4_out = torch.tensor(\n",
    "    prepocess_input_sequence(ex4_out, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    ")\n",
    "\n",
    "ex1_out_expected = torch.tensor([14, 10, 1, 0, 2, 9, 15])\n",
    "ex2_out_expected = torch.tensor([14, 10, 1, 3, 4, 8, 15])\n",
    "ex3_out_expected = torch.tensor([14, 11, 0, 2, 4, 2, 15])\n",
    "ex4_out_expected = torch.tensor([14, 11, 0, 0, 7, 8, 15])\n",
    "\n",
    "print(\n",
    "    \"preprocess input token error 1: \",\n",
    "    rel_error(ex1_processed_expected, ex1_inp_preprocessed),\n",
    ")\n",
    "print(\n",
    "    \"preprocess input token error 2: \",\n",
    "    rel_error(ex2_processed_expected, ex2_inp_preprocessed),\n",
    ")\n",
    "print(\n",
    "    \"preprocess input token error 3: \",\n",
    "    rel_error(ex3_processed_expected, ex3_inp_preprocessed),\n",
    ")\n",
    "print(\n",
    "    \"preprocess input token error 4: \",\n",
    "    rel_error(ex4_processed_expected, ex4_inp_preprocessed),\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\"preprocess output token error 1: \", rel_error(ex1_out_expected, ex1_out))\n",
    "print(\"preprocess output token error 2: \", rel_error(ex2_out_expected, ex2_out))\n",
    "print(\"preprocess output token error 3: \", rel_error(ex3_out_expected, ex3_out))\n",
    "print(\"preprocess output token error 4: \", rel_error(ex4_out_expected, ex4_out))\n",
    "# 所有error都应该为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "579e6f2e-d2f0-4a10-a54f-c65ecdccc587"
   },
   "source": [
    "### 构建DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f2d8dd1"
   },
   "source": [
    "现在，我们将使用实现的位置编码来构建一个PyTorch中的数据加载器。数据加载器的作用是为训练/验证返回一个批次。我们首先创建一个Dataset类，该类为我们提供批次中的单个元素，然后使用DataLoader来封装数据集。我们从`torch.utils.data.Dataset`类继承Dataset。这个类包含两个重要的函数，根据你的用例,你会更改这些函数。第一个函数是`__init__`，这包含了*静态*的组件，换句话说，当我们从完整数据中获取下一个元素时，这些变量不会改变。第二个函数是`__getitem__`，它包含最终数据加载器的核心功能。\n",
    "\n",
    "要获取最终的数据加载器，我们将`train_data`和`test_data`包装在`torch.utils.data.DataLoader`类中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:21:35.117987Z",
     "iopub.status.busy": "2024-04-24T07:21:35.117272Z",
     "iopub.status.idle": "2024-04-24T07:21:35.128433Z",
     "shell.execute_reply": "2024-04-24T07:21:35.127457Z",
     "shell.execute_reply.started": "2024-04-24T07:21:35.117955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AddSubDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_seqs,\n",
    "        target_seqs,\n",
    "        convert_str_to_tokens,\n",
    "        special_tokens,\n",
    "        emb_dim,\n",
    "        pos_encode,\n",
    "    ):\n",
    "\n",
    "        \"\"\"\n",
    "       该类实现了将用于玩具数据集的数据加载器。\n",
    "\n",
    "        参数:\n",
    "        - `input_seqs`: 输入字符串列表\n",
    "        - `target_seqs`: 输出字符串列表\n",
    "        - `convert_str_to_tokens`: 将输入字符串转换为标记的字典\n",
    "        - `special_tokens`: 特殊字符串列表\n",
    "        - `emb_dim`: Transformer的嵌入维度\n",
    "        - `pos_encode`: 用于计算数据的位置编码的函数\n",
    "        \"\"\"\n",
    "\n",
    "        self.input_seqs = input_seqs\n",
    "        self.target_seqs = target_seqs\n",
    "        self.convert_str_to_tokens = convert_str_to_tokens\n",
    "        self.emb_dim = emb_dim\n",
    "        self.special_tokens = special_tokens\n",
    "        self.pos_encode = pos_encode\n",
    "\n",
    "    def preprocess(self, inp):\n",
    "        return prepocess_input_sequence(\n",
    "            inp, self.convert_str_to_tokens, self.special_tokens\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取数据中索引为idx的元素的核心函数。\n",
    "        参数：\n",
    "        - `idx`：需要从数据中提取的元素的索引\n",
    "        返回：\n",
    "        - `preprocess_inp`：长度为K的一维张量，其中K是输入序列的长度\n",
    "        - `inp_pos_enc`：形状为(K, M)的张量，其中K是序列长度，M是嵌入维度\n",
    "        - `preprocess_out`：长度为O的一维张量，其中O是输出序列的长度\n",
    "        - `out_pos_enc`：形状为(O, M)的张量，其中O是序列长度，M是嵌入维度\n",
    "        \"\"\"\n",
    "\n",
    "        inp = self.input_seqs[idx]\n",
    "        out = self.target_seqs[idx]\n",
    "        preprocess_inp = torch.tensor(self.preprocess(inp))\n",
    "        preprocess_out = torch.tensor(self.preprocess(out))\n",
    "        inp_pos = len(preprocess_inp)\n",
    "        inp_pos_enc = self.pos_encode(inp_pos, self.emb_dim)\n",
    "        out_pos = len(preprocess_out)\n",
    "        out_pos_enc = self.pos_encode(out_pos, self.emb_dim)\n",
    "\n",
    "        return preprocess_inp, inp_pos_enc[0], preprocess_out, out_pos_enc[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:21:47.329458Z",
     "iopub.status.busy": "2024-04-24T07:21:47.328550Z",
     "iopub.status.idle": "2024-04-24T07:21:47.339639Z",
     "shell.execute_reply": "2024-04-24T07:21:47.338826Z",
     "shell.execute_reply.started": "2024-04-24T07:21:47.329426Z"
    },
    "id": "a673ee9f-c5ae-438c-b696-9662dcb87c52",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "setup_seed(2024)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "X_train, y_train = train_data[\"inp_expression\"], train_data[\"out_expression\"]\n",
    "X_val, y_val = val_data[\"inp_expression\"], val_data[\"out_expression\"]\n",
    "X_test, y_test = test_data[\"inp_expression\"], test_data[\"out_expression\"]\n",
    "\n",
    "train_data = AddSubDataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    32,\n",
    "    position_encoding_sinusoid\n",
    "    \n",
    ")\n",
    "valid_data = AddSubDataset(\n",
    "    X_val, y_val, convert_str_to_tokens, SPECIAL_TOKENS, 32, position_encoding_sinusoid\n",
    ")\n",
    "test_data = valid_data = AddSubDataset(\n",
    "    X_test, y_test, convert_str_to_tokens, SPECIAL_TOKENS, 32, position_encoding_sinusoid\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-24T07:21:52.084209Z",
     "iopub.status.busy": "2024-04-24T07:21:52.083443Z",
     "iopub.status.idle": "2024-04-24T07:21:52.128366Z",
     "shell.execute_reply": "2024-04-24T07:21:52.126996Z",
     "shell.execute_reply.started": "2024-04-24T07:21:52.084161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "inp_seq_len = 9\n",
    "out_seq_len = 5\n",
    "num_heads = 4\n",
    "emb_dim = 32\n",
    "dim_feedforward = 64\n",
    "dropout = 0.2\n",
    "num_enc_layers = 4\n",
    "num_dec_layers = 4\n",
    "vocab_len = len(vocab)\n",
    "\n",
    "model = Transformer(\n",
    "    num_heads,\n",
    "    emb_dim,\n",
    "    dim_feedforward,\n",
    "    dropout,\n",
    "    num_enc_layers,\n",
    "    num_dec_layers,\n",
    "    vocab_len,\n",
    ")\n",
    "for it in test_loader:\n",
    "  it\n",
    "  break\n",
    "inp, inp_pos, out, out_pos = it\n",
    "device = DEVICE\n",
    "model = model.to(device)\n",
    "inp_pos = inp_pos.to(device)\n",
    "out_pos = out_pos.to(device)\n",
    "out = out.to(device)\n",
    "inp = inp.to(device)\n",
    "\n",
    "# 验证模型的输出维度是否与数据一致\n",
    "model_out = model(inp.long(), inp_pos, out.long(), out_pos)\n",
    "assert model_out.size(0) == BATCH_SIZE * (out_seq_len - 1)\n",
    "assert model_out.size(1) == vocab_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17ac4856-63fd-4693-87b2-ae616f438202"
   },
   "source": [
    "# 第四部分:在AddSub数据集上训练Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62a4b8da"
   },
   "source": [
    "现在我们已经实现了Transformer模型，我们将进行模型的训练。\n",
    "- 通常，Transformer的训练制度会以预热开始，换句话说，我们会使用较低的学习率对模型进行一些迭代训练，然后增加学习率，以使网络更快地学习。直觉上讲，这有助于在损失函数中获得一个稳定的流形，然后我们增加学习率以在这个稳定的流形中更快地学习。从某种意义上讲，我们在预热网络以进入稳定流形，并在此预热后以更高的学习率开始训练。对于过拟合，我们没有使用这种预热，因为对于如此小的数据，可以直接使用较高的学习率，但在使用完整数据进行训练时，您应该记住这一点。我们从a5_helper.py中使用了两个函数，`train`和`val`。在这里，`train`有三个参数需要注意：\n",
    "  - `warmup_interval`：指定网络应该使用较低学习率进行训练的迭代次数。换句话说，它是在网络将使用较高学习率的迭代次数之后的迭代次数。\n",
    "  - `warmup_lr`：这是在预热期间将使用的学习率。\n",
    "  - `lr`：这是在预热后将使用的学习率。如果warmup_interval为None，则我们将以此学习率开始训练。\n",
    "\n",
    "- 运行以下单元格来使用完整数据拟合模型。请记住，在这里你可以尝试各种实验，包括损失函数、位置编码、预热程序和学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def LabelSmoothingLoss(pred, ground):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        pred：预测张量，形状为（N*O，V），其中N是批量大小，O\n",
    "            是目标序列长度，V是词汇表的大小\n",
    "        ground：地面真实张量，形状为（N，O），其中N是批量大小，O\n",
    "            是目标序列\n",
    "    \"\"\"\n",
    "    ground = ground.contiguous().view(-1)\n",
    "    eps = 0.1\n",
    "    n_class = pred.size(1)\n",
    "    one_hot = torch.nn.functional.one_hot(ground).to(pred.dtype)\n",
    "    one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "    log_prb = F.log_softmax(pred, dim=1)\n",
    "    loss = -(one_hot * log_prb).sum(dim=1)\n",
    "    loss = loss.sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def CrossEntropyLoss(pred, ground):\n",
    "    \"\"\"\n",
    "    参数：\n",
    "        pred：预测张量，形状为（N*O，V），其中N是批量大小，O\n",
    "            是目标序列长度，V是词汇表的大小\n",
    "        ground：地面真实张量，形状为（N，O），其中N是批量大小，O\n",
    "            是目标序列\n",
    "    \"\"\"\n",
    "    loss = F.cross_entropy(pred, ground, reduction=\"sum\")\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "inp_seq_len = 9\n",
    "out_seq_len = 5\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "#You should change these!\n",
    "\n",
    "num_heads = 4\n",
    "emb_dim = 32\n",
    "dim_feedforward = 32\n",
    "dropout = 0.2\n",
    "num_enc_layers = 4\n",
    "num_dec_layers = 4\n",
    "vocab_len = len(vocab)\n",
    "loss_func = CrossEntropyLoss\n",
    "poss_enc = position_encoding_sinusoid\n",
    "num_epochs = 10\n",
    "warmup_interval = None\n",
    "lr = 1e-3\n",
    "\n",
    "model = Transformer(\n",
    "    num_heads,\n",
    "    emb_dim,\n",
    "    dim_feedforward,\n",
    "    dropout,\n",
    "    num_enc_layers,\n",
    "    num_dec_layers,\n",
    "    vocab_len,\n",
    ")\n",
    "\n",
    "\n",
    "train_data = AddSubDataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    emb_dim,\n",
    "    position_encoding_sinusoid,\n",
    ")\n",
    "valid_data = AddSubDataset(\n",
    "    X_val,\n",
    "    y_val,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    emb_dim,\n",
    "    position_encoding_sinusoid,\n",
    ")\n",
    "test_data = AddSubDataset(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    convert_str_to_tokens,\n",
    "    SPECIAL_TOKENS,\n",
    "    emb_dim,\n",
    "    position_encoding_sinusoid,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    loss_func,\n",
    "    num_epochs,\n",
    "    batch_size=32,\n",
    "    warmup_lr=6e-6,\n",
    "    warmup_interval=1000,\n",
    "    lr=6e-4,\n",
    "    device=torch.device(\"cpu\"),\n",
    "):\n",
    "    print(\"Training started...\")\n",
    "    if warmup_interval is None:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=lr, betas=(0.9, 0.995), eps=1e-9\n",
    "        )\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=warmup_lr, betas=(0.9, 0.995), eps=1e-9\n",
    "        )\n",
    "    iteration = 0\n",
    "    for epoch_num in range(num_epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_correct = 0\n",
    "        epoch_total = 0\n",
    "        model.train()\n",
    "        for it in train_dataloader:\n",
    "            inp, inp_pos, out, out_pos = it\n",
    "            model = model.to(device)\n",
    "            inp_pos = inp_pos.to(device)\n",
    "            out_pos = out_pos.to(device)\n",
    "            out = out.to(device)\n",
    "            inp = inp.to(device)\n",
    "            gnd = out[:, 1:].contiguous().view(-1).long()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(inp.long(), inp_pos, out.long(), out_pos)\n",
    "            # TRAIN ACC\n",
    "            pred_max = pred.max(1)[1]\n",
    "            gnd = gnd.contiguous().view(-1)\n",
    "            n_correct = pred_max.eq(gnd).sum().item()\n",
    "            epoch_correct += n_correct\n",
    "            epoch_total += len(pred_max)\n",
    "            loss = loss_func(pred, gnd)\n",
    "            epoch_loss.append(loss.item())\n",
    "            if warmup_interval is not None and iteration == warmup_interval:\n",
    "                print(\n",
    "                    f\"End of warmup. Swapping learning rates from {warmup_lr} to {lr}\"\n",
    "                )\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    warmup_lr = lr\n",
    "                    param_group[\"lr\"] = lr\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iteration = iteration + 1\n",
    "\n",
    "        train_acc = epoch_correct / epoch_total\n",
    "        avg_epoch_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "        val_loss, val_acc = val(model, val_dataloader, loss_func, batch_size)\n",
    "        loss_hist = avg_epoch_loss / (batch_size * 4)\n",
    "        print(\n",
    "            f\"[epoch: {epoch_num+1}]\",\n",
    "            \"[loss: \",\n",
    "            f\"{loss_hist:.4f}\",\n",
    "            \"]\",\n",
    "            \"[train_acc: \",\n",
    "            f\"{train_acc:.4f}\",\n",
    "            \"]\",\n",
    "            \"[val_acc: \",\n",
    "            f\"{val_acc:.4f}\",\n",
    "            \"]\",\n",
    "            \"val_loss: [val_loss \",\n",
    "            f\"{val_loss:.4f}\",\n",
    "            \"]\",\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def val(model, dataloader, loss_func, batch_size, device=torch.device(\"cpu\")):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_loss = []\n",
    "        num_correct = 0\n",
    "        total = 0\n",
    "        for it in dataloader:\n",
    "            inp, inp_pos, out, out_pos = it\n",
    "\n",
    "            model = model.to(device)\n",
    "            inp_pos = inp_pos.to(device)\n",
    "            out_pos = out_pos.to(device)\n",
    "            out = out.to(device)\n",
    "            inp = inp.to(device)\n",
    "            gnd = out[:, 1:].contiguous().view(-1).long()\n",
    "            pred = model(inp.long(), inp_pos, out.long(), out_pos)\n",
    "            loss = loss_func(pred, gnd)\n",
    "\n",
    "            pred_max = pred.max(1)[1]\n",
    "            gnd = gnd.contiguous().view(-1)\n",
    "\n",
    "            n_correct = pred_max.eq(gnd)\n",
    "            n_correct = n_correct.sum().item()\n",
    "            num_correct = num_correct + n_correct\n",
    "\n",
    "            total = total + len(pred_max)\n",
    "            epoch_loss.append(loss.item())\n",
    "\n",
    "        avg_epoch_loss = sum(epoch_loss) / len(epoch_loss)\n",
    "    return avg_epoch_loss / (batch_size * 4), n_correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load()))\n",
    "# model.to(DEVICE)\n",
    "trained_model = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    loss_func,\n",
    "    num_epochs,\n",
    "    lr = lr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    warmup_interval=warmup_interval,\n",
    "    device=DEVICE\n",
    ")\n",
    "weights_path = os.path.join('/kaggle/working/', \"transformer.pt\")\n",
    "torch.save(trained_model.state_dict(), weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Final test accuracy\n",
    "print(\n",
    "    \"Final Model accuracy: \",\n",
    "    \"{:.2f}\".format(\n",
    "        val(\n",
    "            trained_model, test_loader, CrossEntropyLoss, 4, device=DEVICE\n",
    "        )[1]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# model.load_state_dict(torch.load(weights_path))\n",
    "# model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请自行调节模型训练的超参数、优化器等，使得模型在测试集上的准确率尽可能高。\n",
    "\n",
    "分段给分（计分时准确率四舍五入保留到小数点后2位，例如0.623计为0.62）：\n",
    "\n",
    "| Baseline  | Accuracy        | Score  |\n",
    "| :---:     |    :----:       |  :---: |\n",
    "| Vallina   | [0.50, 0.57\\)  |   6   |\n",
    "| Simple    | [0.57, 0.64\\)   |   9  |\n",
    "| Medium    | [0.64, 0.71\\)   |   11   |\n",
    "| Hard      | [0.71, 0.80\\)   |   13   |\n",
    "| Strong    | [0.80, 1.00\\)   |   15   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c19615d0-9b82-42ac-ba28-e079ca96aed2"
   },
   "source": [
    "## 可视化与推断：模型展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f413c17-ede5-4d4d-a1fa-54458ca948c5"
   },
   "source": [
    "既然我们已经训练了一个模型，让我们来看看最终的结果。我们将首先查看来自验证数据的结果，并可视化注意力权重（记住self.weights_softmax吗？）。这些注意力权重应该让你对网络学习到了什么有一些直觉。我们已经为您实现了所有内容，意图是帮助您探索模型并了解网络学习到了什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c39117ef-da8b-4fb0-904b-c5c4c37fc800",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def inference(model, inp_exp, inp_exp_pos, out_pos_exp, out_seq_len):\n",
    "    model.eval()\n",
    "    y_init = torch.LongTensor([14]).unsqueeze(0).cuda().view(1, 1)\n",
    "\n",
    "    ques_emb = model.emb_layer(inp_exp)\n",
    "    q_emb_inp = ques_emb + inp_exp_pos\n",
    "    enc_out = model.encoder(q_emb_inp)\n",
    "    for i in range(out_seq_len - 1):\n",
    "        ans_emb = model.emb_layer(y_init)\n",
    "        a_emb_inp = ans_emb + out_pos_exp[:, : y_init.shape[1], :]\n",
    "        dec_out = model.decoder(a_emb_inp, enc_out, None)\n",
    "        _, next_word = torch.max(\n",
    "            dec_out[0, y_init.shape[1] - 1 : y_init.shape[1]], dim=1\n",
    "        )\n",
    "\n",
    "        y_init = torch.cat([y_init, next_word.view(1, 1)], dim=1)\n",
    "    return y_init, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "137296b8-8ab8-4f9d-bff5-e2584370a757"
   },
   "source": [
    "### 验证数据的结果\n",
    "\n",
    "在下面的单元格中，我们选择验证数据中的第一个数据点，并找到其结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37c01cc4-96b0-4a4e-a820-64e5c2dd5549",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for it in valid_loader:\n",
    "    it\n",
    "    break\n",
    "inp, inp_pos, out, out_pos = it\n",
    "opposite_tokens_to_str = {v: k for k, v in convert_str_to_tokens.items()}\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "inp_pos = inp_pos.to(device)\n",
    "out_pos = out_pos.to(device)\n",
    "out = out.to(device)\n",
    "inp = inp.to(device)\n",
    "\n",
    "inp_exp = inp[:1, :]\n",
    "inp_exp_pos = inp_pos[:1]\n",
    "out_pos_exp = out_pos[:1, :]\n",
    "inp_seq = [opposite_tokens_to_str[w.item()] for w in inp_exp[0]]\n",
    "print(\n",
    "    \"Input sequence: \\n\",\n",
    "    inp_seq[0]\n",
    "    + \" \"\n",
    "    + inp_seq[1]\n",
    "    + \" \"\n",
    "    + inp_seq[2]\n",
    "    + inp_seq[3]\n",
    "    + \" \"\n",
    "    + inp_seq[4]\n",
    "    + \" \"\n",
    "    + inp_seq[5]\n",
    "    + \" \"\n",
    "    + inp_seq[6]\n",
    "    + inp_seq[7]\n",
    "    + \" \"\n",
    "    + inp_seq[8],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab535f1d-3be0-40ef-a0fd-8fca2fae2414",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "out_seq_ans, _ = inference(\n",
    "    trained_model, inp_exp, inp_exp_pos, out_pos_exp, out_seq_len\n",
    ")\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "print(\"Output Sequence:\", end=\"\\t\")\n",
    "res = \"BOS \"\n",
    "for i in range(1, out_seq_ans.size(1)):\n",
    "    sym = opposite_tokens_to_str[out_seq_ans[0, i].item()]\n",
    "    if sym == \"EOS\":\n",
    "        break\n",
    "    res += sym + \" \" \n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3412c073-d239-450a-aa46-9ec3d61309a6"
   },
   "source": [
    "### 选择您自己的探索示例\n",
    "\n",
    "在下面的单元格中，您可以输入一个示例到输入样式中，更改变量 `custom_seq`。我们已经为您填写了一个占位表达式，但随意更改它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7d2f3b1-ba94-4ccf-84e3-f3238059f25d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "custom_seq = \"BOS POSITIVE 02 subtract NEGATIVE 07 EOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3caa768d-f61f-4ee4-8e87-259043e93cdb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "out = prepocess_input_sequence(custom_seq, convert_str_to_tokens, SPECIAL_TOKENS)\n",
    "inp_exp = torch.tensor(out).to(DEVICE)\n",
    "\n",
    "out_seq_ans, model_for_visv = inference(\n",
    "    trained_model, inp_exp, inp_exp_pos, out_pos_exp, out_seq_len\n",
    ")\n",
    "\n",
    "trained_model.eval()\n",
    "\n",
    "print(\"Output Sequence:\", end=\"\\t\")\n",
    "res = \"BOS \"\n",
    "for i in range(1, out_seq_ans.size(1)):\n",
    "    sym = opposite_tokens_to_str[out_seq_ans[0, i].item()]\n",
    "    if sym == \"EOS\":\n",
    "        break\n",
    "    res += sym + \" \"\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b9ae8cd-2813-4845-a4df-d47ebdc60971"
   },
   "source": [
    "### 可视化注意力权重\n",
    "\n",
    "在这部分中，我们将为您输入的特定自定义输入可视化注意力权重。编码器和解码器有单独的热图。颜色中的较浅值显示了该行和列中存在的令牌之间的较高关联，而较暗的颜色则显示它们之间的关系较弱。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8e82ff56-d55b-45b7-8507-514ee5968ed5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def draw(data, x, y, ax):\n",
    "    seaborn.heatmap(\n",
    "        data,\n",
    "        xticklabels=x,\n",
    "        square=True,\n",
    "        yticklabels=y,\n",
    "        vmin=0.0,\n",
    "        vmax=1.0,\n",
    "        cbar=False,\n",
    "        ax=ax,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "db22789f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "target_exp = res.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9cae7e26-2587-4297-a44c-c59ab541dfc8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for layer in range(num_enc_layers):\n",
    "    fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
    "    print(\"Encoder Block Number\", layer + 1)\n",
    "    for h in range(num_heads):\n",
    "        draw(\n",
    "            trained_model.encoder.layers[layer]\n",
    "            .multihead_attention.heads[h]\n",
    "            .weights_softmax.data.cpu()\n",
    "            .numpy()[0],\n",
    "            inp_seq,\n",
    "            inp_seq if h == 0 else [],\n",
    "            ax=axs[h],\n",
    "        )\n",
    "    plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be6ba6cb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for layer in range(num_dec_layers):\n",
    "    fig, axs = plt.subplots(1, num_heads, figsize=(20, 10))\n",
    "\n",
    "    print(\"Decoder Block number \", layer + 1)\n",
    "\n",
    "    print(\"Decoder Self Attention\", layer + 1)\n",
    "    for h in range(num_heads):\n",
    "        draw(\n",
    "            trained_model.decoder.layers[layer]\n",
    "            .attention_self.heads[h]\n",
    "            .weights_softmax.data.cpu()\n",
    "            .numpy()[0],\n",
    "            target_exp,\n",
    "            target_exp if h == 0 else [],\n",
    "            ax=axs[h],\n",
    "        )\n",
    "    plt.show()\n",
    "    print(\"Decoder Cross attention\", layer + 1)\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "    for h in range(num_heads):\n",
    "        draw(\n",
    "            trained_model.decoder.layers[layer]\n",
    "            .attention_cross.heads[h]\n",
    "            .weights_softmax.data.cpu()\n",
    "            .numpy()[0],\n",
    "            inp_seq,\n",
    "            target_exp if h == 0 else [],\n",
    "            ax=axs[h],\n",
    "        )\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b43b5e66-7d96-49a7-8d73-649c1d8de2ef",
    "137296b8-8ab8-4f9d-bff5-e2584370a757",
    "3412c073-d239-450a-aa46-9ec3d61309a6",
    "0b9ae8cd-2813-4845-a4df-d47ebdc60971"
   ],
   "name": "Transformers.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3e6a8e772529b48ea93620fbc55d49ea9e469a86dedbc53ac24607b5264d00e5"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4867368,
     "sourceId": 8212761,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
